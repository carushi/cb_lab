{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8bd4738-0d89-4edb-85c0-e5ed071e1615",
   "metadata": {},
   "source": [
    "# CNN-based prediction for the properties at each genomic loci\n",
    "* ResidualBind https://github.com/p-koo/residualbind\n",
    "\n",
    "Dataset:\n",
    "* [BICCN_sub_cluster_and_annotation_file.csv](https://www.dropbox.com/scl/fi/ku2tki1yjhfaq05xkb507/BICCN_sub_cluster_and_annotation_file.csv?rlkey=8ai68ud4dqbbqfljhqw8rv4h9&dl=0) - cluster annotation data\n",
    "* [targets.bed](https://www.dropbox.com/scl/fi/d3c64poskies4owpvqfyu/targets.bed?rlkey=rsyuy487alh7s9sq7xebpjsxo&dl=0) - tab deliminated bed file for 33 clusters\n",
    "* [GRCm38.fa](https://www.dropbox.com/scl/fi/n0u93vm7fhos3rfgf9t6h/GRCm38.fa?rlkey=u1spsgpbkfopr396z1qh8f1q0&dl=0) - mouse genome data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "653fd369-6f20-4083-8867-c9f25b3a64ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b111279-7b0a-4f5d-8e24-ba3463cd7a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Any, List, Callable, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import pyfaidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e945efe1-cb7a-46f0-ac7a-01d29a9d637a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(\n",
    "        sequence: str,\n",
    "        alphabet: str = \"ACGT\",\n",
    "        neutral_alphabet: str = \"N\",\n",
    "        neutral_value: Any = 0,\n",
    "        dtype=np.float32\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"One-hot encode sequence.\"\"\"\n",
    "\n",
    "    def to_uint8(s):\n",
    "        return np.frombuffer(s.encode(\"ascii\"), dtype=np.uint8)\n",
    "\n",
    "    lookup = np.zeros([np.iinfo(np.uint8).max, len(alphabet)], dtype=dtype)\n",
    "    lookup[to_uint8(alphabet)] = np.eye(len(alphabet), dtype=dtype)\n",
    "    lookup[to_uint8(neutral_alphabet)] = neutral_value\n",
    "    lookup = lookup.astype(dtype)\n",
    "    return lookup[to_uint8(sequence)]\n",
    "\n",
    "class WindowedGenomeDataset:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            target_path: str,\n",
    "            fasta_path: str,\n",
    "            window_size: int = None,\n",
    "            chromosomes: List[str] = None,\n",
    "            dtype: np.float32 = None\n",
    "        ):\n",
    "        self.target_path = target_path\n",
    "        self.fasta_path = fasta_path\n",
    "        self.chromosomes = chromosomes\n",
    "        self.dtype = dtype\n",
    "\n",
    "        df = pd.read_table(target_path, header=None, sep=\"\\t\")\n",
    "        c = df[0].isin(chromosomes) if chromosomes else np.array(len(df)*[True])\n",
    "        self.windows = df.loc[c, :2].values\n",
    "        self.targets = df.loc[c, 3:].values.astype(self.dtype)\n",
    "\n",
    "        _, start, stop = self.windows[0]\n",
    "        self.window_size = window_size or (stop - start)\n",
    "\n",
    "        self.input_shape = (self.window_size, 4)\n",
    "        self.output_shape = (self.targets.shape[1],)\n",
    "\n",
    "        self.genome = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, item: Union[int, slice]):\n",
    "        if not self.genome:\n",
    "            from pyfaidx import Fasta\n",
    "            self.genome = Fasta(self.fasta_path)\n",
    "\n",
    "        chrom = self.windows[item, 0]\n",
    "        start = self.windows[item, 1]\n",
    "        stop = self.windows[item, 2]\n",
    "\n",
    "        labels = self.targets[item]\n",
    "\n",
    "        if isinstance(item, slice):\n",
    "            seq = \"\"\n",
    "            for chr, i, j in zip(chrom, start, stop):\n",
    "                seq += self.genome[chr][i:j].seq.upper()\n",
    "\n",
    "            inputs = one_hot_encode(seq)\n",
    "            inputs = inputs.reshape(len(chrom), self.window_size, 4)\n",
    "\n",
    "            item = range(item.start or 0, item.stop or len(self), item.step or 1)\n",
    "            item = np.array(list(item), dtype=np.int32)\n",
    "        else:\n",
    "            seq = self.genome[chrom][start:stop].seq.upper()\n",
    "            inputs = one_hot_encode(seq)\n",
    "\n",
    "            item = item if item >= 0 else len(self) + item\n",
    "\n",
    "        output = {\n",
    "            \"inputs\": inputs,\n",
    "            \"labels\": labels,\n",
    "            \"meta\": {\n",
    "                \"id\": item,\n",
    "                \"chrom\": chrom,\n",
    "                \"start\": start,\n",
    "                \"stop\": stop\n",
    "            }\n",
    "        }\n",
    "        return output\n",
    "\n",
    "    def to_each_record():\n",
    "        for i in tqdm(indices or range(len(self))):\n",
    "            output = self[i]\n",
    "            yield dict(\n",
    "                id=output[\"meta\"][\"id\"],\n",
    "                inputs=output[\"inputs\"],\n",
    "                labels=output[\"labels\"],\n",
    "                chrom=output[\"meta\"][\"chrom\"],\n",
    "                start=output[\"meta\"][\"start\"],\n",
    "                stop=output[\"meta\"][\"stop\"]\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5b2b57b-f2da-4705-b2ab-b7bbd5025c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/\"\n",
    "SAVE_DIR = \"../models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8b2ee5e-1a1b-4756-b80b-450a87db3ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [\n",
    "    (\"train\", [2, 4, 6, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \"X\"]),\n",
    "    (\"valid\", [7, 9]),\n",
    "    (\"test\", [1, 3, 5]),\n",
    "    (\"debug\",[19])\n",
    "]\n",
    "\n",
    "datasets = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c80b252-e6c4-4ec7-baf6-753b8acb8faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 1733242/1733242 [37:00<00:00, 780.70it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 250622/250622 [05:22<00:00, 776.02it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 482812/482812 [10:19<00:00, 779.61it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 56988/56988 [01:13<00:00, 779.09it/s]\n"
     ]
    }
   ],
   "source": [
    "for name, chroms in items:\n",
    "    datasets[name] = WindowedGenomeDataset(\n",
    "        target_path=f\"{DATA_DIR}/targets.bed\",\n",
    "        fasta_path=f\"{DATA_DIR}/GRCm38.fa\",\n",
    "        chromosomes=[f\"chr{i}\" for i in chroms],\n",
    "        window_size=1000,\n",
    "        dtype=np.float32\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "809921a1-3e81-48d2-b116-eaeb522e0413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  1733242 0.7026630169507466\n",
      "Validation:  250622 0.10160312906924136\n",
      "Validation:  482812 0.19573385398001197\n"
     ]
    }
   ],
   "source": [
    "train_size = len(datasets['train'])\n",
    "valid_size = len(datasets['valid'])\n",
    "test_size = len(datasets['test'])\n",
    "\n",
    "total = train_size + valid_size + test_size\n",
    "\n",
    "print(\"Train: \", train_size, train_size / total)\n",
    "print(\"Validation: \", valid_size, valid_size / total)\n",
    "print(\"Validation: \", test_size,  test_size / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5154beac-0724-471b-97f3-2770212f05ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae3fe3a3-b654-4327-a7e6-1716366721b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e2fdff9-49a6-4155-bc71-6b9b4eb59f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "THRESHOLD = 0.0006867924257022952\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20ee29cd-f565-4ab0-9789-aae64e05c5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  0.7026630169507466\n",
      "Validation:  0.10160312906924136\n",
      "Validation:  0.19573385398001197\n"
     ]
    }
   ],
   "source": [
    "train_size = 1733242\n",
    "valid_size = 250622\n",
    "test_size = 482812\n",
    "total = train_size + valid_size + test_size\n",
    "\n",
    "print(\"Train: \", train_size / total)\n",
    "print(\"Validation: \", valid_size / total)\n",
    "print(\"Validation: \", test_size / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fea34d-d6e7-416a-9bdc-589a1032d784",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f0fa7fb-6a7f-4bde-8017-c2106b02a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DilatedResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, filter_size, rates):\n",
    "        super(DilatedResidualBlock, self).__init__()\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels, in_channels, filter_size, dilation=rate, padding=rate)\n",
    "            for rate in rates\n",
    "        ])\n",
    "        self.batch_norms = nn.ModuleList([nn.BatchNorm1d(in_channels) for _ in rates])\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        for conv, bn in zip(self.convs, self.batch_norms):\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            x = conv(x)\n",
    "            x = bn(x)\n",
    "        return F.relu(x + residual)\n",
    "\n",
    "class ResidualBind(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape, activation='exponential', num_units=[128, 256, 512, 512]):\n",
    "        super(ResidualBind, self).__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.num_units = num_units\n",
    "        kernel_size = [19, 9, 7]\n",
    "        self.conv1 = nn.Conv1d(input_shape[0], num_units[0], kernel_size[0], padding='same')\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(num_units[0])\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "\n",
    "        self.res_block1 = DilatedResidualBlock(num_units[0], 3, [1, 2, 4, 8])\n",
    "        self.max_pool1 = nn.MaxPool1d(10)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(num_units[0], num_units[1], kernel_size[1], padding='same')\n",
    "        self.bn2 = nn.BatchNorm1d(num_units[1])\n",
    "        self.dropout3 = nn.Dropout(0.1)\n",
    "\n",
    "        self.res_block2 = DilatedResidualBlock(num_units[1], 3, [1, 2, 4])\n",
    "        self.max_pool2 = nn.MaxPool1d(10)\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(num_units[1], num_units[2], kernel_size[2], padding='same')\n",
    "        self.bn3 = nn.BatchNorm1d(num_units[2])\n",
    "\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dropout5 = nn.Dropout(0.3)\n",
    "\n",
    "        # Fully-connected NN\n",
    "        self.fc1 = nn.Linear(num_units[2], num_units[3])\n",
    "        self.bn4 = nn.BatchNorm1d(num_units[3])\n",
    "        self.dropout6 = nn.Dropout(0.5)\n",
    "        # Output layer\n",
    "        self.fc2 = nn.Linear(num_units[3], output_shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.elu(x) if self.activation == 'exponential' else F.sigmoid(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Dilated Residual Block 1\n",
    "        x = self.res_block1(x)\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Layer 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        # Dilated Residual Block 2\n",
    "        x = self.res_block2(x)\n",
    "        x = self.max_pool2(x)\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        # Layer 3\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        # Global Average Pooling\n",
    "        x = self.global_avg_pool(x).squeeze(-1)\n",
    "        x = self.dropout5(x)\n",
    "\n",
    "        # Fully-connected Layer\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout6(x)\n",
    "\n",
    "        # Output Layer\n",
    "        logits = self.fc2(x)\n",
    "        outputs = torch.sigmoid(logits)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf5f526e-ac7e-4755-9418-b4bbbe2a8180",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualCNN(ResidualBind):\n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.elu(x) if self.activation == 'exponential' else F.sigmoid(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Layer 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        # Layer 3\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        # Global Average Pooling\n",
    "        x = self.global_avg_pool(x).squeeze(-1)\n",
    "        x = self.dropout5(x)\n",
    "\n",
    "        # Fully-connected Layer\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout6(x)\n",
    "\n",
    "        # Output Layer\n",
    "        logits = self.fc2(x)\n",
    "        outputs = torch.sigmoid(logits)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6afdf5af-fb4b-4ac2-9034-89ef6a9b01ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7239207029342651\n",
      "Outputs: tensor([[0.3542],\n",
      "        [0.5610],\n",
      "        [0.3922],\n",
      "        [0.5928],\n",
      "        [0.4082],\n",
      "        [0.5311],\n",
      "        [0.2784],\n",
      "        [0.5975],\n",
      "        [0.5943],\n",
      "        [0.2659],\n",
      "        [0.4483],\n",
      "        [0.5138],\n",
      "        [0.4478],\n",
      "        [0.5573],\n",
      "        [0.4650],\n",
      "        [0.6773]], grad_fn=<SigmoidBackward0>)\n",
      "Loss: 0.007446830160915852\n",
      "Outputs: tensor([[9.8996e-01],\n",
      "        [9.9431e-01],\n",
      "        [8.4643e-03],\n",
      "        [1.7581e-02],\n",
      "        [1.7528e-03],\n",
      "        [3.2361e-02],\n",
      "        [9.9916e-01],\n",
      "        [9.9776e-01],\n",
      "        [9.9863e-01],\n",
      "        [9.9642e-01],\n",
      "        [1.5486e-03],\n",
      "        [5.8025e-04],\n",
      "        [1.0967e-03],\n",
      "        [9.7920e-01],\n",
      "        [9.9910e-01],\n",
      "        [9.9077e-01]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# ハイパーパラメータ\n",
    "input_shape = (4, 1000)  # 入力データの形状 (チャンネル数, データ長)\n",
    "output_shape = 1         # 出力ユニット数\n",
    "batch_size = 16          # バッチサイズ\n",
    "\n",
    "# モデルの初期化\n",
    "model = ResidualBind(input_shape=input_shape, output_shape=output_shape)\n",
    "model.train()\n",
    "\n",
    "# サンプルデータの作成 (ランダムなデータ)\n",
    "inputs = torch.randn(batch_size, input_shape[0], input_shape[1])  # (バッチサイズ, チャンネル数, データ長)\n",
    "labels = torch.randint(0, 2, (batch_size, output_shape)).float()  # (バッチサイズ, 出力ユニット数)\n",
    "\n",
    "# 損失関数とオプティマイザの設定\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion = torch.nn.BCELoss()  # バイナリクロスエントロピー損失関数\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for i in range(100):\n",
    "    # フォワードパス\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # 損失の計算\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # バックプロパゲーションと最適化ステップ\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 結果の表示\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Loss: {loss.item()}\")\n",
    "        print(f\"Outputs: {outputs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5672ba66-a15a-4d27-9714-de6e83e7924e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7579466700553894\n",
      "Outputs: tensor([[0.4504],\n",
      "        [0.4385],\n",
      "        [0.5576],\n",
      "        [0.2630],\n",
      "        [0.3705],\n",
      "        [0.4902],\n",
      "        [0.4480],\n",
      "        [0.2338],\n",
      "        [0.5474],\n",
      "        [0.6590],\n",
      "        [0.5761],\n",
      "        [0.5698],\n",
      "        [0.4914],\n",
      "        [0.3898],\n",
      "        [0.5079],\n",
      "        [0.7020]], grad_fn=<SigmoidBackward0>)\n",
      "Loss: 0.0028502282220870256\n",
      "Outputs: tensor([[9.9836e-01],\n",
      "        [2.3870e-03],\n",
      "        [9.9879e-01],\n",
      "        [1.1728e-02],\n",
      "        [5.0695e-03],\n",
      "        [1.0171e-03],\n",
      "        [9.9935e-01],\n",
      "        [9.9879e-01],\n",
      "        [9.9666e-01],\n",
      "        [1.6517e-03],\n",
      "        [6.9901e-04],\n",
      "        [4.6731e-03],\n",
      "        [2.3526e-03],\n",
      "        [9.8954e-04],\n",
      "        [1.8836e-03],\n",
      "        [4.9718e-03]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# ハイパーパラメータ\n",
    "input_shape = (4, 1000)  # 入力データの形状 (チャンネル数, データ長)\n",
    "output_shape = 1         # 出力ユニット数\n",
    "batch_size = 16          # バッチサイズ\n",
    "\n",
    "# モデルの初期化\n",
    "model = ResidualCNN(input_shape=input_shape, output_shape=output_shape)\n",
    "model.train()\n",
    "\n",
    "# サンプルデータの作成 (ランダムなデータ)\n",
    "inputs = torch.randn(batch_size, input_shape[0], input_shape[1])  # (バッチサイズ, チャンネル数, データ長)\n",
    "labels = torch.randint(0, 2, (batch_size, output_shape)).float()  # (バッチサイズ, 出力ユニット数)\n",
    "\n",
    "# 損失関数とオプティマイザの設定\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion = torch.nn.BCELoss()  # バイナリクロスエントロピー損失関数\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for i in range(100):\n",
    "    # フォワードパス\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # 損失の計算\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # バックプロパゲーションと最適化ステップ\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 結果の表示\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Loss: {loss.item()}\")\n",
    "        print(f\"Outputs: {outputs}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geneformer",
   "language": "python",
   "name": "geneformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
